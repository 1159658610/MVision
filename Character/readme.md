# 文字识别 Optical Character Recognition,OCR

将图片上的文字内容，智能识别成为可编辑的文本。

> 场景文字识别（Scene Text Recognition，STR）

OCR（Optical Character Recognition, 光学字符识别）传统上指对输入扫描文档图像进行分析处理，识别出图像中文字信息。场景文字识别（Scene Text Recognition，STR） 指识别自然场景图片中的文字信息。自然场景图像中的文字识别，其难度远大于扫描文档图像中的文字识别，因为它的文字展现形式极其丰富：

·允许多种语言文本混合，字符可以有不同的大小、字体、颜色、亮度、对比度等。
·文本行可能有横向、竖向、弯曲、旋转、扭曲等式样。
·图像中的文字区域还可能会产生变形(透视、仿射变换)、残缺、模糊等现象。
·自然场景图像的背景极其多样。如文字可以出现在平面、曲面或折皱面上；文字区域附近有复杂的干扰纹理、或者非文字区域有近似文字的纹理，比如沙地、草丛、栅栏、砖墙等。


也有人用OCR技术泛指所有图像文字检测和识别技术， 包括传统OCR技术与场景文字识别技术。这是因为，场景文字识别技术可以被看成是传统OCR技术的自然演进与升级换代。



> **应用:**

身份证、名片、银行卡、户口本等卡证类、出版物(扫描版图像、试题)、票据类(发票、火车票、彩票、出租车票)的印刷体识别；

运单、考试试卷、办公手写文档、快递手写单号等手写体识别；

车牌、集装箱号、快递运单、行驶证、驾驶证、等交通物流字符识别等；

水表、电表、燃气表等各种传感器可视化数据识别(5G物联网之后可能就不需要了);

图像文字检测和识别技术有着广泛的应用场景。已经被互联网公司落地的相关应用涉及了识别名片、识别菜单、识别快递单、识别身份证、识别营业证、识别银行卡、识别车牌、识别路牌、识别商品包装袋、识别会议白板、识别广告主干词、识别试卷、识别单据等等。

文本检测和识别技术处于一个学科交叉点，其技术演进不断受益于计算机视觉处理和自然语言处理两个领域的技术进步。它既需要使用视觉处理技术来提取图像中文字区域的图像特征向量，又需要借助自然语言处理技术来解码图像特征向量为文字结果。


## 什么是OCR？

OCR英文全称是Optical Character Recognition，中文叫做光学字符识别。它是利用光学技术和计算机技术把印在或写在纸上的文字读取出来，并转换成一种计算机能够接受、人又可以理解的格式。文字识别是计算机视觉研究领域的分支之一，而且这个课题已经是比较成熟了，并且在商业中已经有很多落地项目了。比如汉王OCR、百度OCR、阿里OCR、腾讯OCR等等，很多企业都有能力都是拿OCR技术开始挣钱了。其实我们自己也能感受到，OCR技术确实也在改变着我们的生活：比如一个手机APP就能帮忙扫描名片、身份证，并识别出里面的信息；汽车进入停车场、收费站都不需要人工登记了，都是用车牌识别技术；我们看书时看到不懂的题，拿个手机一扫，APP就能在网上帮你找到这题的答案。太多太多的应用了，OCR的应用在当今时代确实是百花齐放啊。

## OCR的发展

在一些简单环境下OCR的准确度已经比较高了（比如电子文档），但是在一些复杂环境下的字符识别，在当今还没有人敢说自己能做的很好。现在大家都很少会把目光还放在如何对电子文档的文字识别该怎么进一步提高准确率了，因为他们把目光放在更有挑战性的领域。OCR传统方法在应对复杂图文场景的文字识别显得力不从心，越来越多人把精力都放在研究如何把文字在复杂场景读出来，并且读得准确作为研究课题，用学界术语来说，就是场景文本识别（文字检测+文字识别）。自然场景下的文字识别比简单场景的文字识别实在困难太多了，现在虽然出了很多成果，但是离理想结果还是差很远。


## OCR的分类

如果要给OCR进行分类，我觉得可以分为两类：**手写体识别和印刷体识别**。这两个可以认为是OCR领域两个大主题了，当然印刷体识别较手写体识别要简单得多，我们也能从直观上理解，印刷体大多都是规则的字体，因为这些字体都是计算机自己生成再通过打印技术印刷到纸上。在印刷体的识别上有其独特的干扰：在印刷过程中字体很可能变得断裂或者墨水粘连，使得OCR识别异常困难。当然这些都可以通过一些图像处理的技术帮他尽可能的还原，进而提高识别率。总的来说，单纯的印刷体识别在业界已经能做到很不错了，但说100%识别是肯定不可能的，但是说识别得不错那是没毛病。

印刷体已经识别得不错了，那么手写体呢？手写体识别一直是OCR界一直想攻克的难关，但是时至今天，感觉这个难关还没攻破，还有很多学者和公司在研究。为什么手写体识别这么难识别？因为人类手写的字往往带有个人特色，每个人写字的风格基本不一样，虽然人类可以读懂你写的文字，但是机器缺很难。那为什么机器能读懂印刷体？因为印刷体是机器造出来的啊，那机器当然能读懂自己造的字体啦哈哈~其实上面也提到了，印刷体一般都比较规则，字体都基本就那几十种，机器学习这几十种字体并不是一件难事，但是手写体，每个人都有一种字体的话，那机器该学习多少字体啊？这就是难度所在。

如果按识别的内容来分类，也就是按照识别的语言的分类的话，那么要识别的内容将是人类的所有语言**（汉语、英语、德语、法语等）**。如果仅按照我们国人的需求，那识别的内容就包括：**汉字、英文字母、阿拉伯数字、常用标点符号**。根据要识别的内容不同，识别的难度也各不相同。简单而言，识别数字是最简单了，毕竟要识别的字符只有0~9，而英文字母识别要识别的字符有26个（如果算上大小写的话那就52个），而中文识别，要识别的字符高达数千个（二级汉字一共6763个）！因为汉字的字形各不相同，结构非常复杂（比如带偏旁的汉字）如果要将这些字符都比较准确地识别出来，是一件相当具有挑战性的事情。但是，并不是所有应用都需要识别如此庞大的汉字集，比如车牌识别，我们的识别目标仅仅是数十个中国各省和直辖市的简称，难度就大大减少了。当然，在一些文档自动识别的应用是需要识别整个汉字集的，所以要保证识别的整体的识别还是很困难的。

传统OCR一般有 模板匹配的方法(简单的场景 单一数字识别)、特征设计提取分类(传统机器学习方法)

## 现代 OCR 流程
深度学习的出现，让OCR技术焕发第二春。现在OCR基本都用卷积神经网络来做了，而且识别率也是惊人的好，人们也不再需要花大量时间去设计字符特征了。在OCR系统中，人工神经网络主要充当特征提取器和分类器的功能，输入是字符图像，输出是识别结果，一气呵成。

* 1.图像预处理(做角度矫正和去噪)

[传统opencv 轮廓检测+透视变换+二值化](https://www.cnblogs.com/skyfsm/p/7324346.html)

最后总结一下两个算法的应用场景：

> 基于轮廓提取的矫正算法更适用于车牌、身份证、人民币、书本、发票一类矩形形状而且边界明显的物体矫正。

> 基于直线探测的矫正算法更适用于文本类的矫正。

[基于轮廓和直线的图片校正](https://www.cnblogs.com/skyfsm/p/6902524.html)

[cnn计算图像透视变换系数 Spatial Transformer Network(STN) ](https://arxiv.org/pdf/1506.02025.pdf)

对于弯曲不规则文本，如果按照之前的识别方法，直接将整个文本区域图像强行送入CNN+RNN，由于有大量的无效区域会导致识别效果很差。所以这篇文章提出一种通过**STN网络Spatial Transformer Network(STN)**学习变换参数，将Rectified Image对应的特征送入后续RNN中识别。

对于STN网络，可以学习一组点 (x_i^s,y_i^s) 到对应点 (x_i^t,y_i^t) 的变换。而且STN可以插入轻松任意网络结构中学习到对应的变换。

    (x_i^s,
    y_i^s)    =  (c11, c12, c13
                  c21, c22, c23)    *  (x_i^t,
                                        y_i^t,
                                        1) 
                                        
* 2.字符检测(行分割/列分 解决的问题是哪里有文字，文字的范围)

传统算法：行切割（水平投影依据像素值(0为黑色)判断行起止）+列切割（垂直投影）
    
    比如“刺”字被分为两部分了，那么我们就直接将这两个“字”送去识别，结果当然是得到一个置信度很低的一个反馈，那么我们就将这两个部分往他们身边最近的、而且没被成功识别的部分进行合并，再将这个合并后的字送进OCR识别，这样子我们就可以通过识别反馈来完成汉字的正确分割和识别了。
    
目标检测相关算法：yolo/ssd/frcnn
    
    准确度还比较高

* 3.字符识别(单个字符识别/序列字符识别)

对定位好的文字区域进行识别，主要解决的问题是每个文字是什么，将图像中的文字区域进转化为字符信息.  

cnn + rnn(lstm) + attention

cnn + rnn(lstm) + CTC

* 4.后处理识别矫正(语法检测器，检测字符的组合逻辑是否合理)




# 基础网络 

图文识别任务中充当特征提取模块的基础网络，可以来源于通用场景的图像分类模型。例如，VGGNet，ResNet、InceptionNet、DenseNet、Inside-Outside Net、Se-Net等。

图文识别任务中的基础网络，也可以来源于特定场景的专用网络模型。

例如，**擅长提取图像细节特征的FCN网络，**

**擅长做图形矫正的STN网络。**

## FCN网络

全卷积网络（FCN,fully convolutional network）， 是去除了全连接(fc)层的基础网络，最初是用于实现语义分割任务。FC的优势在于利用反卷积（deconvolution）、上池化（unpooling）等上采样（upsampling）操作，将特征矩阵恢复到接近原图尺寸，然后对每一个位置上的像素做类别预测，从而能识别出更清晰的物体边界。基于FCN的检测网络，不再经过候选区域回归出物体边框, 而是根据高分辨率的特征图直接预测物体边框。因为不需要像Faster-RCNN那样在训练前定义好候选框长宽比例，FCN在预测不规则物体边界时更加鲁棒。由于FCN网络最后一层特征图的像素分辨率较高，而图文识别任务中需要依赖清晰的文字笔画来区分不同字符（特别是汉字），所以FCN网络很适合用来提取文本特征。当FCN被用于图文识别任务时，最后一层特征图中每个像素将被分成**文字行（前景）和非文字行（背景）两个类别。**


## STN网络

空间变换网络（STN，Spatial Transformer Networks）的作用是对输入特征图进行空间位置矫正得到输出特征图，这个矫正过程是可以进行梯度传导的，从而能够支持端到端的模型训练。

如下图所示，STN网络由定位网络（Localization Network） ，网格生成器（Grid generator），采样器（Sampler）共3个部分组成。定位网络根据原始特征图U计算出一套控制参数，网格生成器这套控制参数产生采样网格（sampling grid），采样器根据采样网格核函数将原始图U中像素对应采样到目标图V中。

空间变换的控制参数是根据原始特征图U动态生成的，生成空间变换控制参数的元参数则是在模型训练阶段学习到的、并且存放于定位网络的权重（weights）矩阵中。


## 检测网络框架

Faster RCNN作为一个检测网络框架，其目标是寻找紧凑包围被检测对象的边框（BBOX，Bounding Box）。如下图所示，它在Fast RCNN检测框架基础上引入区域建议网络（RPN，Region Proposal Network），来快速产生与目标物体长宽比例接近的多个候选区域参考框（anchor）；它通过ROI（Region of Interest） Pooling层为多种尺寸参考框产生出归一化固定尺寸的区域特征；它利用共享的CNN卷积网络同时向上述RPN网络和ROI Pooling层输入特征映射（Feature Maps），从而减少卷积层参数量和计算量。训练过程中使用到了多目标损失函数，包括RPN网络、ROI Pooling层的边框分类loss和坐标回归loss。通过这些loss的梯度反向传播，能够调节候选框的坐标、并增大它与标注对象边框的重叠度/交并比(IOU，Intersection over Union）。RPN网格生成的候选框初始值有固定位置以及长宽比例。如果候选框初始长宽比例设置得与图像中物体形状差别很大，就很难通过回归找到一个紧凑包围它的边框。


SSD（Single Shot MultiBox Detector），是2016年提出的一种全卷积目标检测算法，截止到目前仍是主要的目标检测框架之一，相比Faster RCNN有着明显的速度优势。如下图所示，SSD是一种one stage算法，直接预测被检测对象的边框和得分。检测过程中，SSD算法利用多尺度思想进行检测，在不同尺度的特征图(feature maps)上产生与目标物体长宽比例接近的多个默认框(Default boxes)，进行回归与分类。最后利用非极大值抑制(Non-maximum suppression)得到最终的检测结果。训练过程中，SSD采用Hard negative mining策略进行训练，使正负样本比例保持为1：3，同时使用多种数据增广(Data augmentation)方式进行训练，提高模型性能。


## 文本检测模型

文本检测模型的目标是从图片中尽可能准确地找出文字所在区域。

但是，视觉领域常规物体检测方法(SSD, YOLO, Faster-RCNN等)直接套用于文字检测任务效果并不理想， 主要原因如下：

1·相比于常规物体，文字行长度、长宽比例变化范围很大。
2·文本行是有方向性的。常规物体边框BBox的四元组描述方式信息量不充足。
3·自然场景中某些物体局部图像与字母形状相似，如果不参考图像全局信息将有误报。
4·有些艺术字体使用了弯曲的文本行，而手写字体变化模式也很多。
5·由于丰富的背景图像干扰，手工设计特征在自然场景文本识别任务中不够鲁棒。

针对上述问题根因，近年来出现了各种基于深度学习的技术解决方案。它们从特征提取、区域建议网络(RPN)、多目标协同训练、Loss改进、非极大值抑制（NMS）、半监督学习等角度对常规物体检测方法进行改造，极大提升了自然场景图像中文本检测的准确率。例如：

1·CTPN方案中，用BLSTM模块提取字符所在图像上下文特征，以提高文本块识别精度。
2·RRPN等方案中，文本框标注采用BBOX +方向角度值的形式，模型中产生出可旋转的文字区域候选框，并在边框回归计算过程中找到待测文本行的倾斜角度。
3·DMPNet等方案中，使用四边形（非矩形）标注文本框，来更紧凑的包围文本区域。
4·SegLink  将单词切割为更易检测的小文字块，再预测邻近连接将小文字块连成词。
5·TextBoxes等方案中，调整了文字区域参考框的长宽比例，并将特征层卷积核调整为长方形，从而更适合检测出细长型的文本行。
6·FTSN方案中，作者使用Mask-NMS代替传统BBOX的NMS算法来过滤候选框。
7·WordSup方案中，采用半监督学习策略，用单词级标注数据来训练字符级文本检测模型。



